<!DOCTYPE html>
<html>
<head>
<title>draft2_impPoints.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="points-from-the-paper">Points from the PAPER</h1>
<h2 id="summary-of-the-paper">SUMMARY of the paper:</h2>
<p>This research paper is about finding ways to make large language models, like ChatGPT, generate objectionable content.</p>
<p>Basically - it adds specific words to a variety of queries in order to make the model produce inappropriate responses.
Unlike previous methods that required human effort, this new method automatically creates these manipulative phrases using computer techniques.</p>
<p>Notably, the harmful phrases created using this approach can work not only on the model they were designed for but also on other similar models, even if the creators of those models tried to make them safe.</p>
<p>This research raises concerns about how to prevent such models from generating harmful information and suggests that better safeguards are needed.</p>
<p>.................................................</p>
<p><em>NOTE</em></p>
<p>LLMs are trained using a lot of text from the internet, which can sometimes include inappropriate content. To make these models safe, researchers have been working on &quot;aligning&quot; them, which means making sure they don't produce harmful or bad responses when people interact with them</p>
<p>To make these models work better for real people, developers use techniques to adjust or &quot;align&quot; them</p>
<p>One way is to fine-tune the model by giving it specific tasks with instructions, so it gets better at following directions. Another way is to teach the model using feedback from humans. This helps the model learn what people like and prefer.</p>
<p>In simple terms, developers make these big language models better at understanding and behaving like we want them to by giving them special training and feedback.</p>
<p>.....................................................</p>
<h2 id="1-introduction">1. INTRODUCTION</h2>
<p>Type of attack :</p>
<ol>
<li>
<p>Specific prompts that can make aligned LLMs produce inappropriate content, but they require huge human effort.</p>
</li>
<li>
<p>Finding more efficient ways to make LLMs generate objectionable content and also testing if these prompts work on different LLMs.</p>
</li>
</ol>
<p>This raises concerns about how to prevent such problems and highlights that there's still work to be done to make sure these models behave well.</p>
<h3 id="this-papers-work">This paper's work</h3>
<p>The approach involves adding a specific set of words to a user's query, causing the model to generate negative or harmful outputs. This method is achieved through three main steps:</p>
<ul>
<li>
<p>Initial Affirmative Responses: The attack prompts the model to start its response in a specific way(&quot;SURE,HERE IS...&quot;), creating a context where objectionable content follows. This is more like triggering something.</p>
</li>
<li>
<p>Greedy and Gradient-Based Optimization: The attack optimizes the added words by using gradients to identify promising replacement tokens. It's similar to a technique called AutoPrompt but more exhaustive, searching over all possible tokens.</p>
</li>
<li>
<p>Robust Multi-Prompt and Multi-Model Attacks: To make the attack effective across different situations, the method is designed to work with various prompts and models, ensuring a wider range of harmful outputs.</p>
</li>
</ul>
<p>By combining the three elements of their approach, the researchers are able to create reliable methods for making language models produce harmful content, effectively bypassing the efforts to align the models. For instance, their method successfully induces objectionable behaviors in benchmark tests, such as generating 99 out of 100 harmful behaviors in the Vicuna model and achieving 88 out of 100 exact matches with a specific harmful string.</p>
<h2 id="2-a-universal-attack-on-llms">2 A Universal Attack on LLMs</h2>
<p>The authors introduce a special set of words (adversarial suffix) to the user's question that tricks the model into giving a response to the original harmful query. This additional text aims to make the model ignore its safety rules and provide the harmful information. The key idea is to find the right words in the adversarial suffix so that the model always responds positively to any question from the user.</p>
<p>This approach involves making choices about how to optimize the adversarial suffix, including the loss function, data, and optimization process.</p>
<h2 id="21-initial-affirmative-responses">2.1 Initial Affirmative Responses</h2>
<p>The intuition of this approach is that if the language model can be put into a “state” where this completion is the most likely response, as opposed to refusing to answer the query, then it likely will continue the completion with precisely the desired objectionable behavior</p>
<p>The manual approach is only marginally successful, though, and can often be circumvented by slightly more sophisticated alignment techniques.</p>
<p>previous work found that - specifying only the first target token was often sufficient. in the text-only space, targeting just the first token runs the risk of entirely overriding the original promp</p>
<p>Formalizing the adversarial objective- see the pdf</p>
<p><img src="file:///Users/rajdeepghosh/Desktop/IITKGP_all/1stSem/seminar/p1.png" alt="image"></p>
<h2 id="22-greedy-goordinate-gradient-based-search">2.2 Greedy Goordinate Gradient-based Search</h2>
<p>The motivation for our approach comes from the greedy coordinate descent approach: if we could evaluate all possible single-token substitutions, we could swap the token that maximally decreased the loss.</p>
<p>Of course, evaluating all such replacements is not feasible, but we can leverage gradients with respect to the one-hot token indicators to find a set of promising candidates for replacement at each token position, and then evaluate all these replacements exactly via a forward pass</p>
<p>Note that because LLMs typically form embeddings for each token, they can be written as functions of this value exi, and thus we can immediately take the gradient with respect to this quantity</p>
<p>We then compute the top-k values with the largest negative gradient as the candidate replacements for token xi. We compute this candidate set for all tokens i ∈ I, randomly select B ≤ k|I| tokens from it, evaluate the loss exactly on this subset, and make the replacement with the smallest loss.</p>
<p><img src="file:///Users/rajdeepghosh/Desktop/IITKGP_all/1stSem/seminar/p2.png" alt="image"></p>
<h2 id="23-universal-multi-prompt-and-multi-model-attacks">2.3 Universal Multi-prompt and Multi-model attacks</h2>
<ol>
<li>
<p>Instead of specifying different subsets of tokens to modify for each prompt, a single postfix (a sequence of tokens added to the end) is optimized. The algorithm aggregates gradients and losses for this postfix. Gradients are clipped to have a unit norm before aggregation. This ensures that the optimization process focuses on the most impactful changes.</p>
</li>
<li>
<p>The algorithm incrementally incorporates new prompts during the optimization process. This means that the algorithm starts by focusing on one prompt and identifies a candidate adversarial example. Once a working adversarial example is found for the first prompt, the process includes another prompt, and so on. This incremental approach is found to be more effective than trying to optimize all prompts simultaneously from the start.</p>
</li>
<li>
<p>Transferable Adversarial Examples: To make the adversarial examples transferable across different models, the algorithm incorporates loss functions that involve multiple models</p>
</li>
</ol>
<h2 id="experimental-results">Experimental results</h2>
<p>There are basically two default settings :
Harmful strings &amp; Harmful behavious</p>
<h2 id="metrics">Metrics:</h2>
<p>Primary : ASR</p>
<p>Secondary: cross-entropy loss on the target string</p>
<h3 id="baseline--pez-wen-et-al-2023-gbda-guo-et-al-2021-and-autoprompt-shin-et-al-2020">Baseline : PEZ [Wen et al., 2023], GBDA [Guo et al., 2021], and AutoPrompt [Shin et al., 2020].</h3>
<h2 id="results">RESULTS</h2>
<p>result
<img src="file:///Users/rajdeepghosh/Desktop/IITKGP_all/1stSem/seminar/p3.png" alt="image"></p>
<h2 id="31-attacks-on-white-box-models">3.1 Attacks on White-box Models</h2>
<p>there are two setups: targeting specific harmful strings and behaviors on a single model, and creating universal attacks on a single model.</p>
<ol>
<li>
<p>1 model -  attack method on a Vicuna-7B model and a LLaMA-2-7B-Chat model. Focus on harmful strings and harmful behaviors.
//comparison of results. (GCG, is more effective in making language models produce harmful content in specific scenarios compared to other methods.)</p>
</li>
<li>
<p>multi model - GCG, performs better than other methods on both train and test scenarios for most cases. On one model, AutoPrompt is somewhat competitive, but on another model, it's much less effective compared to GCG. GCG achieves high success rates, particularly on LLaMA-2-7B-Chat.</p>
</li>
</ol>
<p><img src="file:///Users/rajdeepghosh/Desktop/IITKGP_all/1stSem/seminar/p4.png" alt="img"></p>
<h2 id="32-transfer-attacks">3.2 Transfer attacks</h2>
<p>// GRAPH (pic)</p>
<p>They create universal adversarial prompts by optimizing a single set of words to work on multiple models and prompts. They use their GCG method to optimize the prompt for two models (Vicuna-7B and 13B) over 25 harmful behaviors. They repeat the process twice with different random seeds to get two attack prompts. They also create a third prompt by including two additional models (Guanaco-7B and 13B) over the same 25 prompts. They select the prompt with the lowest loss after 500 optimization steps for each run.</p>
<p>Comparison</p>
<p>The researchers evaluate the transferability of their attack method on various models using 388 test harmful behaviors. Their attack performs better than other methods across open-source models and even surpasses proprietary models like GPT-3.5 and GPT-4. The attack's success rate is almost 100% for some models.</p>
<p>.............</p>
<p>In summary, the key distinction is that &quot;attack on white-box models&quot; focuses on evaluating the attack on models it was designed for, while &quot;transfer attack&quot; evaluates the attack's effectiveness on models it wasn't explicitly tailored to.</p>
<h2 id="enhancing-transferability">Enhancing transferability.</h2>
<p>The researchers enhance the attack's transferability by combining multiple GCG prompts. Concatenating three GCG prompts into one suffix increases ASR, especially on GPT-3.5, although it has varying effects on different models. Additionally, an ensemble approach that uses multiple GCG prompts improves ASR significantly, achieving high success rates on GPT-3.5 and notable rates on GPT-4 and Claude-1.</p>
<p>In summary, combining prompts and using an ensemble approach helps the attack achieve high success rates, particularly on open-source models and even black-box models like GPT-4, showcasing the attack's effectiveness on harmful behaviors.</p>
<p>&lt; Instead of relying on just one instance of the attack, an ensemble approach combines the results from several instances, each optimized slightly differently, to enhance the overall performance of the attack.&gt;</p>
<p>Fine tuning  :  changing &quot;Generate instructions&quot; to &quot;Create a tutorial&quot; worked well. This manual adjustment improved success rates. They also used similar strategies on other models, like Claude 2, to make the attack work better.</p>
<h1 id="conclusion">CONCLUSION</h1>
<p>In conclusion, despite limited progress in constructing reliable  attacks against modern language models (LLMs), this paper introduces a simple yet effective approach that significantly advances the state of practical attacks. This method builds upon existing techniques, yielding substantial success in circumventing alignment training of LLMs.</p>
<p>Future research in this area holds many questions. Can models be explicitly fine-tuned to defend against these attacks? Adversarial training might offer a solution. Will this lead to robust models while maintaining their generative capabilities? Can increased standard alignment training mitigate the issue? Furthermore, can pre-training mechanisms prevent such behaviors altogether? These are important questions for advancing the field and ensuring the robustness of language models.</p>
<h1 id="qs">Qs</h1>
<ol>
<li>Idea on the code</li>
<li>Loss function in algo 3 &amp; not quite sure how does involve multiple models</li>
<li></li>
</ol>

</body>
</html>
